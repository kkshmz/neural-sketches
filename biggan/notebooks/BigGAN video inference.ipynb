{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import linalg as LA\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model_size = \"3)biggan-512\" #@param [\"1)biggan-128\" , \"2)biggan-256\" , \"3)biggan-512\"]\n",
    "which_model = model_size.split(')')[1]\n",
    "module_path = 'https://tfhub.dev/deepmind/'+which_model+'/2'\n",
    "module = hub.Module(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {k: tf.placeholder(v.dtype, v.get_shape().as_list(), k)\n",
    "          for k, v in module.get_input_info_dict().items()}\n",
    "output = module(inputs)\n",
    "\n",
    "print ('Inputs:\\n', '\\n'.join(\n",
    "        '{}: {}'.format(*kv) for kv in inputs.items()))\n",
    "\n",
    "print ('Output:', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the number of the labels and the size of the latent space $z$ to sample from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = inputs['y'].get_shape().as_list()[1]\n",
    "latent_size = inputs['z'].get_shape().as_list()[1]\n",
    "\n",
    "print('Number of labels ', vocab_size)\n",
    "print('The size of the latent space ', latent_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model takes label inputs as one hot encoded which maps each class ùëê‚àà[0,1000) to a vector of size 1000 with all zeros except the index of the corrosponding class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions to generate the encoding\n",
    "def one_hot(index, vocab_size=1000):\n",
    "    index = np.asarray(index)\n",
    "    if len(index.shape) == 0:\n",
    "        index = np.asarray([index])\n",
    "    assert len(index.shape) == 1\n",
    "    num = index.shape[0]\n",
    "    output = np.zeros((num, vocab_size), dtype=np.float32)\n",
    "    output[np.arange(num), index] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncation Trick\n",
    "\n",
    "Previous work on GANs samples the latent vector ùëß‚àºÓà∫(0,ùêº) as a normal distirubtion with the identity convariance matrix. OTOH, the authors of BigGans used a truncated normal distriubtion in a certain region [‚àíùëé,ùëé] for ùëé‚àà‚Ñù+where the sampled values outside that region are resampled to fall in the region. This resulted in better results of both IS and FID scores. The drawback of this is a reduction in the overall variety of vector sampling. Hence there is a trade-off between sample quality and variety ofr a given network G.\n",
    "\n",
    "Here we set the default truncation threshold to be 2 i.e ùëß values are sampled from the region [‚àí2,2]. Note that the optional variable seed takes integer values and it's used to reserve the state of randomness for resampling. Hence, if you use the same seed you will get the same ùëß vector ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zy(index, trunc = 1., batch_size = 1, seed = None):\n",
    "    #convert the label to one hot encoding \n",
    "    y = one_hot(index)\n",
    "\n",
    "    #sample a batch of z-vectors \n",
    "    z = truncnorm.rvs(-2, 2, size=(batch_size, latent_size), random_state = np.random.RandomState(seed)) * trunc\n",
    "    return z, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(img):\n",
    "    img = np.clip(((img + 1) / 2.0) * 256, 0, 255)\n",
    "    img = np.uint8(img)  \n",
    "    img = img.squeeze()\n",
    "    return img\n",
    "def generate(sess, z, y, trunc = 1.):\n",
    "    feed_dict = {inputs['z']: z, inputs['y']:y, inputs['truncation']: trunc}\n",
    "    im = sess.run(output, feed_dict=feed_dict)\n",
    "\n",
    "    #postprocess the image \n",
    "    im = postprocess(im)\n",
    "    return im\n",
    "\n",
    "def interpolate_hypersphere(v1, v2, num_steps):\n",
    "    v1_norm = LA.norm(v1)\n",
    "    v2_norm = LA.norm(v2)\n",
    "    v2_normalized = v2 * (v1_norm / v2_norm)\n",
    "\n",
    "    vectors = []\n",
    "    for step in range(num_steps):\n",
    "        interpolated = v1 + (v2_normalized - v1) * step / (num_steps - 1)\n",
    "        interpolated_norm =  LA.norm(interpolated)\n",
    "        interpolated_normalized = interpolated * (v1_norm / interpolated_norm)\n",
    "        vectors.append(interpolated_normalized)\n",
    "    return np.array(vectors)\n",
    "\n",
    "def get_zy(index, seed = None):\n",
    "    yA = one_hot(index)\n",
    "    zA = truncnorm.rvs(-2, 2, size=(1, z_dim), random_state = np.random.RandomState(seed))\n",
    "    return zA, yA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new part\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "def classify_image(classifier, img):\n",
    "    h, w = hub.get_expected_image_size(classifier)\n",
    "    x = tf.placeholder(tf.float32, shape=(None, h, w, 3))\n",
    "    y = tf.nn.softmax(classifier(x))\n",
    "    data = transform.resize(img, [h, w])\n",
    "    with tf.Session(config=config).as_default() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        y_pred = sess.run(y, feed_dict={x: [data]})\n",
    "        return y_pred\n",
    "classifier = hub.Module(\"https://tfhub.dev/google/imagenet/nasnet_large/classification/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "for img in os.listdir():\n",
    "    y_pred = classify_image(classifier, img)\n",
    "    im0 = cv2.resize(img,(512, 512))\n",
    "    im1 = sample(sess, z, y, truncation=truncation)[0]\n",
    "    ims = imgrid(np.array([im0, im1]), cols=2)\n",
    "    images.append(ims)\n",
    "    imshow(ims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
